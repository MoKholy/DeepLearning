<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>Project Title Here</title>

  <!-- Bootstrap core CSS -->
  <link href="../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

</head>

<body>

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-dark bg-dark static-top">
    <div class="container">
      <a class="navbar-brand" href="../home.html">Practical Machine Deep Learning</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item active">
            <a class="nav-link" href="../home.html">Home
              <span class="sr-only">(current)</span>
            </a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="../about.html">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="../contact.html">Contact</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Page Content -->
  <div class="container">
    <div class="row">
      <div class="col-lg-12 text-center">
        <h1 class="mt-5">Music Genre Classification</h1>
        <ul class="list-unstyled">
          <li>Mohammad ElKholy</li>
          <li>Allaa ElKhouly</li>
        </ul>
      </div>
    </div>


    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">Problem Statement</h2>
		<p>
			With the increasing popularity of social media platforms, music streaming apps, and the need for music analysis, the accurate classification of music genres has become crucial. Music genres provide valuable information about the style, mood, and characteristics of a particular song, allowing users to discover new music and curate personalized playlists. However, existing tagging and classification algorithms often fall short in accurately categorizing songs into their respective genres.
      <br/> 
      <br/>
      The problem at hand is the need to improve the accuracy and efficiency of music genre classification algorithms, specifically tailored for social media platforms, music streaming apps, and music analysis. Current systems face several challenges in accurately tagging songs with appropriate genres, leading to inaccurate recommendations, playlist compositions, and music analysis results. 
<br/> 
<br/>
Tagging plays a crucial role in music genre classification as it enables the organization, retrieval, and recommendation of music based on genre preferences. Accurate tagging allows users to easily search for songs, create personalized playlists, and receive relevant music recommendations. Moreover, accurate genre tagging facilitates music analysis for researchers and industry professionals, enabling them to gain insights into consumer preferences, market trends, and popular music styles.
<br/> 
<br/>
Social media platforms and music streaming apps have experienced exponential growth in recent years, resulting in a vast amount of user-generated content. Likewise, music streaming apps heavily rely on accurate genre classification to deliver tailored music recommendations, enhance user engagement, and provide an improved listening experience.
<br/> 
<br/>
Beyond user-centric applications, music genre classification has significant implications for music analysis and research. Accurate classification of music genres allows researchers and professionals to study music consumption patterns, identify emerging genres, understand cross-genre influences, and develop new music recommendation algorithms. It contributes to a deeper understanding of the music landscape and aids in making informed decisions regarding music production, marketing, and promotion.
<br/> 
 <br/>
		</p>
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">Dataset</h2>

        <p>
          Initially, we used Free Music Archive dataset, the small version, which is a new one that contains 8 different genres and 1000 music pieces for each genre making in balanced dataset. However a problem that we faced is that the quality of the music in the dataset was different within the genre itself. Even models that gave very good results in GTZAN, which is benchmark in literature, gave very bad results on this dataset. Even after performing data augmentation, we still got very bad results. We thus decided to switch use GTZAN dataset instead.

          <br/> 
          <br/>
          
          The GTZAN dataset has 10 genres.

          <ul>
              <li> Blues </li>
              <li> Classic </li>
              <li> Country </li>
              <li> Hip-Hop </li>
              <li> Disco </li>
              <li> Jazz </li>
              <li> Pop </li>
              <li> Rock </li>
              <li> Metal </li>
              <li> Reggae </li>
            </ul>
        </p>
        
        <h2 class="mt-5">Data PreProcessing</h2>
        <p>
          The datset provides files in .wav format. We used librosa to read the files and extract the features. We used the following features for the log mel spectrogram:
          <ul>
            <li> Sampling Rate: 22050 Hz </li>
            <li> Frame Size: 2048   </li>
            <li> Hop Lengh: 512 </li>
            <li> Number of Mel Bins: 128 </li>
        </p>
        <h2 class="mt-5">Data Augmentation</h2>
        <p>
          Our original dataset was still small and the model did not converge initially, we thus used data augmentation to increase the size of the dataset. We increased the size of the GTZAN dataset spliting each clip into 3 segments of 10 seconds each. We used an 80/10/10 split for train/validation/test respectively. 
 
        </p>


		<br/> <!-- Empty Line before the image -->
	    <div class="img-container" align="left"> <!-- Block parent element -->
          <audio controls>
            <source src="resources/audio/000002.mp3" type="audio/mpeg">
            Your browser does not support the audio element.
          </audio>
          <h6>Genre: Hip-Hop</h6>
    	</div>
      <div class="img-container" align="left"> <!-- Block parent element -->
        <audio controls>
          <source src="resources/audio/000005.mp3" type="audio/mpeg">
          Your browser does not support the audio element.
        </audio>
        <h6>Genre: Hip-Hop</h6>
    </div>

    	<br/> <!-- Empty Line after the image -->
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">Input/Output Examples</h2>

        <p>
			Our input starts by being a simple audio file as shown below. 
		</p>
    <div align = "center">
     <audio controls align="center">
      <source src="resources/audio/jazz.00008.wav" type="audio/mpeg" align="center">
      Your browser does not support the audio element.
     </audio>
   </div>
   <div class="row">
    <div class="col-lg-12 text-left">

      <p>
    Then the audio file is converted to a spectrogram. The spectrograms below were generated using different paramenter. 
  </p>
		<br/> <!-- Empty Line before the image -->
	    <div class="img-container" align="center"> <!-- Block parent element -->
      		<img src="resources/images/Untitled design.png" class="img-fluid text-center">
    	</div>
    	<br/> <!-- Empty Line after the image -->
      <p>
        The spectrogram is then fed to the model, and the output is a probability distribution over the 10 classes.
        <br/>
        <br/>
        <br/>

        Output: Jazz
      </p>
      <p>
      </p>
      </div>
    </div>



    <!-- <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">State of the art</h2>

        <p>
			State here state of the art models and their accuracies, you might need to do this by including images of tables
		</p>

		<br/> <!-- Empty Line before the image -->
	    <!-- <div class="img-container" align="center"> <!-- Block parent element -->
      		<!-- <img src="resources/images/state-of-the-art-results.png" class="img-fluid text-center">
    	</div>
    	<br/> <!-- Empty Line after the image -->
      <!-- </div>
    </div> --> 

    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">Orignial Model from Literature</h2>

        <p>
          <h4>AST Model</h4>
		</p>

		<br/> <!-- Empty Line before the image -->
	    <div class="img-container" align="center"> <!-- Block parent element -->
      		<img src="resources/images/unnamed.png" class="img-fluid text-center">
    	</div>
    	<br/> <!-- Empty Line after the image -->
      </div>
    </div>

    <div class="row">
      <div class="col-lg-12 text-left">

        <p>
          The Audio Spectrogram Transformer (AST) model is a deep learning architecture designed specifically for audio-related tasks, such as music genre classification. It combines the power of transformers and spectrogram representations to learn complex patterns and relationships within audio data.
          <br>
          <br>
          The AST model processes audio signals by first converting them into spectrograms, which are visual representations of the frequencies and amplitudes present in a sound wave over time. The spectrograms are then treated as images and fed into a transformer architecture, which consists of multiple layers of self-attention mechanisms. The self-attention mechanisms enable the model to capture long-range dependencies and contextual information from the spectrogram, allowing it to learn discriminative features for genre classification.
          <br>
          <br>
                    During training, the AST model showed promising results, achieving high accuracy and demonstrating its capability to learn intricate patterns from the available data. However, when it came to generalization, the AST model faced challenges. It did not perform as well on unseen data, indicating a lack of robustness.
          <br>
          <br>
          The main reason for the limited generalization of the AST model was the requirement for a large amount of data to match or surpass the performance of convolutional neural network (CNN) architectures commonly used for audio tasks. Unfortunately, the available datasets did not contain enough information to fully leverage the potential of AST. Even with data augmentation techniques applied, the model's generalization performance remained unsatisfactory.
          <br>
          <br>
          Another factor contributing to the generalization issue was the lack of computational power required to train larger AST models effectively. As the model size increases, so does the need for computational resources to handle the increased complexity. Without adequate computational power, it becomes challenging to train larger models that can capture a more comprehensive representation of audio features. 
          <br>
          <br>
      </p>
      </div>
      <h4>Bottom-up Broadcast</h4>
      <br/> <!-- Empty Line before the image -->
	    <div class="img-container" align="center"> <!-- Block parent element -->
      		<img src="resources/images/bottom.png" class="img-fluid text-center">
    	</div>
    	<br/> <!-- Empty Line after the image -->

      <p>
       
       The Bottom-up Broadcast Neural Network (BBNN) is a neural network architecture specifically designed for music genre classification tasks. Unlike traditional feed-forward neural networks, the BBNN takes a bottom-up approach, analyzing music signals at various temporal scales to capture both local and global patterns.
       <br>
       <br>
       The BBNN architecture consists of multiple layers, including convolutional layers, pooling layers, and fully connected layers. The network initially processes the input audio clips using convolutional layers, which extract local temporal features from short segments of the audio. The pooling layers aggregate the extracted features, capturing more global information about the audio signals. Finally, the fully connected layers perform classification based on the learned representations.
       <br>
       <br>
       In the literature, the BBNN model has shown promising results in music genre classification tasks. It has been able to achieve high accuracy, with reported results of up to 93.9%. However, it is important to note that these results were obtained using audio clips with a length of 30 seconds. Below are the reults achieved by the BBNN model on the GTZAN dataset in the literature which is considered state of the art.
       <br>
       <br>
       <div class="img-container" align="center"> <!-- Block parent element -->
        <img src="resources/images/BBNN.png" class="img-fluid text-center">
    </div>
       <br>
       

      </p>

    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">Proposed Updates</h2>
        <br/>
        <br/>
        <h4> <u>AST</u></h4>
        <h5 class="mt-5">Update #1: Music Genre Classification </h5>
        <br/> 
        The AST has never been used with music genre classification before. We trained it on Free Music Archieve and the GTZAN datset to see if it improves; however none had enough data even with data augmentaion. 
        <br/>
        <h5 class="mt-5">Update #2: Fine Tuning AST </h5>
        <br/>
        We fined tuned AST using pretrained weights of Audio Set dataset.
        <div class="row">
          <div class="col-lg-12 text-left">
            <h2 class="mt-5">Results</h2>
    
            <p>
          Below is the confusion matrix of the AST without pretraining.
        </p>
    
        <br/> <!-- Empty Line before the image -->
          <div class="img-container" align="center"> <!-- Block parent element -->
              <img src="resources/images/withoutpre.png" class="img-fluid text-center">
          </div>
          <br/> <!-- Empty Line after the image -->
          </div>
        </div>
        <p>
          Below is the confusion matrix of the AST with pretraining.
        </p>
        <br/> <!-- Empty Line before the image -->
        <div class="img-container" align="center"> <!-- Block parent element -->
            <img src="resources/images/withpre.png" class="img-fluid text-center">
        </div>
        <br/> <!-- Empty Line after the image -->
        </div>
      </div>
    
        <br>
        <br>
        <h4> <u>BBNN</u></h4>
        <h5 class="mt-5">Update #1: Did Data Augmentation </h5>
        
       To further improve the performance of the BBNN model and make it more suitable for real-world applications, we performed data augmentation techniques were applied. In this case, the length of the audio clips was shortened from 30 seconds to 10 seconds. After data augmentation, the BBNN model achieved even more impressive results, with training accuracy reaching up to 99% and validation accuracy up to 94% compared to the literatute that achieved 93.9%.
       <br>
       <br>
       The use of data augmentation, specifically shortening the length of the clips, likely contributed to the higher accuracy achieved by the BBNN model. By reducing the length of the clips, the model can capture more concise and relevant information, which may enhance its ability to classify music genres accurately.
       <div class="row">
        <div class="col-lg-12 text-left">
          <h2 class="mt-5">Results</h2>
  
          <p>
        Below is the confusion matrix of the BBNN with data augmentation.
      </p>
  
      <br/> <!-- Empty Line before the image -->
        <div class="img-container" align="center"> <!-- Block parent element -->
            <img src="resources/images/BNNAUG.png" class="img-fluid text-center">
        </div>
        <br/> <!-- Empty Line after the image -->
        </div>
       <br/>
       <br>
       <h5 class="mt-5">Update #2: Applied a GRU block  </h5>
      
        Lastly, we decided to add a GRU block to the BBNN architecture after the first convolutional layer. The model was able to achieve a training accuracy 82% on the validation set. 
        </div>
        <div class="row">
          <div class="col-lg-12 text-left">
            <h2 class="mt-5">Results</h2>
    
            <p>
          Below is the confusion matrix of the BBNN with data augmentation and a GRU block.
        </p>
    
        <br/> <!-- Empty Line before the image -->
          <div class="img-container" align="center"> <!-- Block parent element -->
              <img src="resources/images/BBNNRNN.png" class="img-fluid text-center">
          </div>
          <br/> <!-- Empty Line after the image -->
          </div>
        </div>
    

  

    

    <div class="row">
      <div class="col-lg-12 text-left">
        <h2 class="mt-5">Technical report</h2>

        <p>
			Here you will detail the details related to training, for example:
		</p>

	 	<ul>
    <h5> Programming Framework</h5>
    <li>Python</li>
    <h5> Training Hardware</h5>
    <li> Two RTX 2080 Supers </li>
    <h5> Training Time</h5>
    <li>BBNN with data augmention: 25 minites to train with early stopping at 90 epochs.</li>
    <li>BBNN with GRU block: 50 minited to train with early stopping at 69 epochs.</li>
    <h5> Number of Epochs</h5>
    <li> 100 Epochs for all of the models</li>
    <h5> Time per Epoch</h5>
    <li> BBNN with data augmentaion: 15 seconds per epoch</li>
    <li> BBNN with GRU block: 30 seconds per epoch</li>
    <h5> Any other important detail or difficulties</h5>
		  
		</ul> 
      </div>
    </div>

	<div class="row">
	  <div class="col-lg-12 text-left">
	    <h2 class="mt-5">Conclusion</h2>

	    <p>
			Conclusion and future work (including lessons learned and interesting findings
		</p>

	  </div>
	</div>

	<div class="row">
	  <div class="col-lg-12 text-left">
	    <h2 class="mt-5">References</h2>

	    <p>
	    	List all references here, the following are only examples
	    </p>

		<ol>
		  <li><a href="http://www.google.com">Problem statement url</a></li>
		  <li><a href="http://www.google.com">Dataset URL</a></li>
		  <li><a href="http://www.google.com">Model Reference URL</a></li>
		</ol> 
	  </div>
	</div>

  </div>



  <!-- Bootstrap core JavaScript -->
  <script src="../vendor/jquery/jquery.slim.min.js"></script>
  <script src="../vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

</body>

</html>
